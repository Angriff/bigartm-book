{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic model for EUR-lex collection classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Murat Apishev, great-mel@yandex.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and experiment description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd learn a topic model of EUR-lex collection in this experiment. It has two modalities --- regular tokens and class labels, and has next features (after pre-processing):\n",
    "- 20000 documents, about 18000 of them are in the train sample (in batches, each batch contains 1000 documents), and about 1950 are in test sample in single batch;\n",
    "- 21000 regular tokens in the vocabulary;\n",
    "- 3900 classes;\n",
    "- each document is refers to 3-6 classes in average.\n",
    "\n",
    "The goal of the experiment is to create a topic model for classification with high quality. The quality measures are:\n",
    "- the area under ROC curve (AUC-ROC)\n",
    "- the area under precision-recall curve (AUC-PR)\n",
    "- the percent of documents with the wrong most probable assigned label (OneError)\n",
    "- the percent of documents without perfect classification (IsError)\n",
    "- an average precision: for each right label we count the part of right labels, that were ranged higher than given one, and after that we average this value first across the labels in document and than across al l documents (AvecPrec).\n",
    "\n",
    "AUC-measures are counted between the vector of probabilities of classes for the document and the vector of right answers for one document and than all values are average across all documents.\n",
    "\n",
    "All described measures are counted on test sample, the documents of test batch doesn't contain information about their class labels.\n",
    "\n",
    "All measures were got from article T. Rubin, A. Chambers, P. Smyth, M. Steyvers: Statistical topic models for multi-label document classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The steps of the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first let's include all necessary Python packages and new BigARTM API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import numpy\n",
    "import sklearn.metrics\n",
    "\n",
    "from artm.artm_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define two help functions that will be used for some quality measures counting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perfect_classification(true_labels, probs):\n",
    "    temp_true_labels = list(true_labels)\n",
    "    temp_probs = list(probs)\n",
    "    for i in xrange(sum(true_labels)):\n",
    "        idx = temp_probs.index(max(temp_probs))\n",
    "        \n",
    "        if temp_true_labels[idx] == 0:\n",
    "            return False\n",
    "        \n",
    "        del temp_true_labels[idx]\n",
    "        del temp_probs[idx]\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_precision(true_labels, probs):\n",
    "    retval, index = 0, -1\n",
    "    for label in true_labels:\n",
    "        denominator, numerator = 0, 0\n",
    "        index += 1\n",
    "        if label:\n",
    "            for prob_idx in xrange(len(probs)):\n",
    "                if probs[prob_idx] > probs[index]:\n",
    "                    denominator += 1\n",
    "                    if true_labels[prob_idx] == 1:\n",
    "                        numerator += 1\n",
    "        if denominator > 0:\n",
    "            retval += numerator / denominator\n",
    "    retval /= sum(true_labels)\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define several helpful static constants. They are:\n",
    "- the name of the modalities (ones, that were used by parser during creation of batches and diciotnary);\n",
    "- the full path to folder containing batches;\n",
    "- the full name of file with information about labels for test documents;\n",
    "- the name of file with '.batch_test' extension containing test documents;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_class = '@labels_class'\n",
    "tokens_class = '@default_class'\n",
    "\n",
    "data_folder         = 'D:/Work/University/course_work/bigartm/multimodal_experiments/eurlex_data'\n",
    "test_labels_file    = os.path.join(data_folder, 'test_labels.eurlex_artm')\n",
    "test_documents_file = '7d6a65e7-712a-43e5-bdad-529075961598.batch_test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the information about labels of test documents at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(test_labels_file, 'rb') as f:\n",
    "    true_p_cd = [[int(p_cd) for p_cd in p_d] for p_d in pickle.load(f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will be defined by set of the parameters. They are\n",
    "- the number of topics\n",
    "- the number of iterations over whole collection\n",
    "- the number of iterations over single document (+)\n",
    "- the weight of the modality \"class labels\" (+)\n",
    "- the weight of the modality \"tokens\" (+)\n",
    "- the coefficient of smoothing of Theta matrix (+)\n",
    "- the coefficient of smoothing of Phi matrix (+)\n",
    "- the coefficient of smoothing of Psi matrix (+)\n",
    "- the coefficient of LabelRegularization regularizer (+)\n",
    "\n",
    "(+) --- means that the variable is a list of values, each value for one iteration of collection scan.\n",
    "\n",
    "Besides this key values we also need to define one technical variable --- list with numbers of iterations, on which the quality measures will be counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics            = 500\n",
    "num_collection_passes = 10\n",
    "\n",
    "num_document_passes   = [16] * num_collection_passes\n",
    "labels_class_weight   = [1.0, 1.0, 0.9, 0.9, 0.9, 0.8, 0.8, 0.8, 0.7, 0.7]\n",
    "tokens_class_weight   = [1] * num_collection_passes\n",
    "\n",
    "smooth_theta_tau      = [0.02] * num_collection_passes\n",
    "smooth_phi_tau        = [0.01] * num_collection_passes\n",
    "\n",
    "smooth_psi_tau        = [0.01] * num_collection_passes\n",
    "label_psi_tau         = [0.0] * num_collection_passes\n",
    "\n",
    "count_scores_iters = [num_collection_passes - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the model and initialize it with the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ArtmModel(num_topics=num_topics, num_document_passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_dictionary(dictionary_name='dictionary', dictionary_path=os.path.join(data_folder, 'dictionary.eurlex_artm'))\n",
    "model.initialize(dictionary_name='dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create the regularizers of smoothig for all three matrices and the LableRegularization regularizer for Psi matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.regularizers.add(SmoothSparsePhiRegularizer(name='SmoothPsiRegularizer', class_ids=[labels_class]))\n",
    "model.regularizers.add(LabelRegularizationPhiRegularizer(name='LabelPsiRegularizer', class_ids=[labels_class]))\n",
    "\n",
    "model.regularizers.add(SmoothSparsePhiRegularizer(name='SmoothPhiRegularizer', class_ids=[tokens_class]))\n",
    "model.regularizers.add(SmoothSparseThetaRegularizer(name='SmoothThetaRegularizer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to start model learning. During each scan of the collection we'll update the values of coefficients of the regularization, the weights of the modalities and the number of iterations over single document. After that we will call the learning method. If we need to find the values of quality measures on this iterations, next operations will be performed:\n",
    "- create Theta matrix for test batch according to current model state;\n",
    "- extract the Psi matrix and count the values of labels in documents as p(c|d) = sum_t p(c|t) * p(t|d);\n",
    "- call functions defined earlier or from sklearn package with p(c|d) vectors and right answers as input data;\n",
    "- average all results across test documents and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for iter in xrange(num_collection_passes):\n",
    "    print 'Iter #' + str(iter)\n",
    "    model.regularizers['SmoothPsiRegularizer'].tau = smooth_psi_tau[iter]\n",
    "    model.regularizers['LabelPsiRegularizer'].tau = label_psi_tau[iter]\n",
    "    model.regularizers['SmoothPhiRegularizer'].tau = smooth_phi_tau[iter]\n",
    "    model.regularizers['SmoothThetaRegularizer'].tau = smooth_theta_tau[iter]\n",
    "    \n",
    "    model.class_ids = {tokens_class: tokens_class_weight[iter], labels_class: labels_class_weight[iter]}\n",
    "\n",
    "    model.num_document_passes = num_document_passes[iter]\n",
    "\n",
    "    model.fit_offline(num_collection_passes=1, data_path=data_folder)\n",
    "    \n",
    "    test_theta = model.find_theta(data_path=data_folder, batches=[test_documents_file])\n",
    "    Psi = model.get_phi(class_ids=[labels_class]).as_matrix()\n",
    "    \n",
    "    items_auc_roc, items_auc_pr = [], []\n",
    "    one_error, is_error, precision = 0, 0, 0\n",
    "    \n",
    "    if iter in count_scores_iters:\n",
    "        print 'Start processing iteration #' + str(iter) + '...'\n",
    "        for item_index in xrange(len(test_theta.columns)):\n",
    "            p_cd = [numpy.dot(test_theta[item_index], p_w) for p_w in Psi]\n",
    "\n",
    "            items_auc_roc.append(sklearn.metrics.roc_auc_score(true_p_cd[item_index], p_cd))\n",
    "            prec, rec, _ = sklearn.metrics.precision_recall_curve(true_p_cd[item_index], p_cd)\n",
    "            items_auc_pr.append(sklearn.metrics.auc(rec, prec))\n",
    "\n",
    "            if true_p_cd[item_index][p_cd.index(max(p_cd))] == 0:\n",
    "                one_error += 1\n",
    "\n",
    "            if not perfect_classification(true_p_cd[item_index], p_cd):\n",
    "                is_error += 1\n",
    "\n",
    "            precision += count_precision(true_p_cd[item_index], p_cd)\n",
    "\n",
    "        average_auc       = sum(items_auc_roc) / len(items_auc_roc)\n",
    "        average_auc_pr    = sum(items_auc_pr) / len(items_auc_roc)\n",
    "        average_one_error = (one_error / len(items_auc_roc)) * 100\n",
    "        average_is_error  = (is_error / len(items_auc_roc)) * 100\n",
    "        average_precision = precision / len(items_auc_roc)\n",
    "\n",
    "        print \"AUC-ROC = %.3f \" % average_auc,\n",
    "        print \"| OneError = %.1f \" % average_one_error,\n",
    "        print \"| IsError = %.1f \" % average_is_error,\n",
    "        print \"| AverPrec = %.3f \" % average_precision,\n",
    "        print \"| AUC-PR = %.3f\" % average_auc_pr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
